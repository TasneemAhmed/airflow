2025-04-17 14:55:56,793 INFO - Loaded executor: SequentialExecutor
2025-04-17 14:55:56,829 INFO - Starting the scheduler
2025-04-17 14:55:56,830 INFO - Processing each file at most -1 times
2025-04-17 14:55:56,835 INFO - Launched DagFileProcessorManager with pid: 355665
2025-04-17 14:55:56,836 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 14:55:56,839 INFO - Configured default timezone UTC
2025-04-17 14:59:27,107 INFO - Setting next_dagrun for executing_Python_pipeline_using_variable to None, run_after=None
2025-04-17 14:59:27,158 INFO - 2 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
2025-04-17 14:59:27,159 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 14:59:27,159 INFO - DAG executing_Python_pipeline_using_variable has 1/16 running and queued tasks
2025-04-17 14:59:27,159 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
2025-04-17 14:59:27,161 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [scheduled]>, <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 14:59:27,161 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='scheduled__2025-04-16T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 14:59:27,162 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'scheduled__2025-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 14:59:27,162 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 14:59:27,162 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 14:59:27,168 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'scheduled__2025-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 14:59:29,266 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 14:59:33,177 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='scheduled__2025-04-16T00:00:00+00:00', try_number=1, map_index=-1)
2025-04-17 14:59:33,178 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1)
2025-04-17 14:59:33,186 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T11:59:26.254367+00:00, map_index=-1, run_start_date=2025-04-17 11:59:31.070713+00:00, run_end_date=2025-04-17 11:59:32.506624+00:00, run_duration=1.435911, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 11:59:27.160507+00:00, queued_by_job_id=201, pid=357553
2025-04-17 14:59:33,190 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=scheduled__2025-04-16T00:00:00+00:00, map_index=-1, run_start_date=2025-04-17 11:59:28.581505+00:00, run_end_date=2025-04-17 11:59:28.798087+00:00, run_duration=0.216582, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 11:59:27.160507+00:00, queued_by_job_id=201, pid=357510
2025-04-17 14:59:36,289 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
2025-04-17 14:59:36,290 INFO - DAG executing_Python_pipeline_using_variable has 1/16 running and queued tasks
2025-04-17 14:59:36,290 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
2025-04-17 14:59:36,292 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 14:59:36,292 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
2025-04-17 14:59:36,293 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 14:59:36,301 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 14:59:38,576 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1)
2025-04-17 14:59:38,582 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T11:59:26.254367+00:00, map_index=-1, run_start_date=2025-04-17 11:59:37.809754+00:00, run_end_date=2025-04-17 11:59:38.027860+00:00, run_duration=0.218106, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 11:59:36.291316+00:00, queued_by_job_id=201, pid=357655
2025-04-17 14:59:38,734 ERROR - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-16 00:00:00+00:00: scheduled__2025-04-16T00:00:00+00:00, state:running, queued_at: 2025-04-17 11:59:27.096723+00:00. externally triggered: False> failed
2025-04-17 14:59:38,735 INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-16 00:00:00+00:00, run_id=scheduled__2025-04-16T00:00:00+00:00, run_start_date=2025-04-17 11:59:27.119008+00:00, run_end_date=2025-04-17 11:59:38.735025+00:00, run_duration=11.616017, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-04-16 00:00:00+00:00, data_interval_end=2025-04-16 00:00:00+00:00, dag_hash=532dc0ac6ff5d311e9df40bcbefaa877
2025-04-17 14:59:38,737 INFO - Setting next_dagrun for executing_Python_pipeline_using_variable to None, run_after=None
2025-04-17 14:59:38,740 ERROR - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 11:59:26.254367+00:00: manual__2025-04-17T11:59:26.254367+00:00, state:running, queued_at: 2025-04-17 11:59:26.289429+00:00. externally triggered: True> failed
2025-04-17 14:59:38,740 INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 11:59:26.254367+00:00, run_id=manual__2025-04-17T11:59:26.254367+00:00, run_start_date=2025-04-17 11:59:27.119085+00:00, run_end_date=2025-04-17 11:59:38.740717+00:00, run_duration=11.621632, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 11:59:26.254367+00:00, data_interval_end=2025-04-17 11:59:26.254367+00:00, dag_hash=532dc0ac6ff5d311e9df40bcbefaa877
2025-04-17 15:00:03,100 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [scheduled]>
2025-04-17 15:00:03,100 INFO - DAG executing_Python_Branching_tasks_with_Backfill has 0/16 running and queued tasks
2025-04-17 15:00:03,100 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [scheduled]>
2025-04-17 15:00:03,102 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:00:03,102 INFO - Sending TaskInstanceKey(dag_id='executing_Python_Branching_tasks_with_Backfill', task_id='task_choice', run_id='scheduled__2025-04-17T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 4 and queue default
2025-04-17 15:00:03,102 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_Branching_tasks_with_Backfill', 'task_choice', 'scheduled__2025-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_cron_catchup_backfill.py']
2025-04-17 15:00:03,108 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_Branching_tasks_with_Backfill', 'task_choice', 'scheduled__2025-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_cron_catchup_backfill.py']
2025-04-17 15:00:04,800 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_Branching_tasks_with_Backfill', task_id='task_choice', run_id='scheduled__2025-04-17T00:00:00+00:00', try_number=1, map_index=-1)
2025-04-17 15:00:04,804 INFO - TaskInstance Finished: dag_id=executing_Python_Branching_tasks_with_Backfill, task_id=task_choice, run_id=scheduled__2025-04-17T00:00:00+00:00, map_index=-1, run_start_date=2025-04-17 12:00:04.101440+00:00, run_end_date=2025-04-17 12:00:04.245905+00:00, run_duration=0.144465, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=210, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-04-17 12:00:03.101358+00:00, queued_by_job_id=201, pid=357898
2025-04-17 15:00:07,999 INFO - Marking run <DagRun executing_Python_Branching_tasks_with_Backfill @ 2025-04-17 00:00:00+00:00: scheduled__2025-04-17T00:00:00+00:00, state:running, queued_at: 2025-04-17 12:00:01.390467+00:00. externally triggered: False> successful
2025-04-17 15:00:08,000 INFO - DagRun Finished: dag_id=executing_Python_Branching_tasks_with_Backfill, execution_date=2025-04-17 00:00:00+00:00, run_id=scheduled__2025-04-17T00:00:00+00:00, run_start_date=2025-04-17 12:00:01.418888+00:00, run_end_date=2025-04-17 12:00:08.000439+00:00, run_duration=6.581551, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-04-17 00:00:00+00:00, data_interval_end=2025-04-17 12:00:00+00:00, dag_hash=a7b8b386f36c9437c41d66ad90406d9d
2025-04-17 15:00:08,007 INFO - Setting next_dagrun for executing_Python_Branching_tasks_with_Backfill to 2025-04-17 12:00:00+00:00, run_after=2025-04-18 00:00:00+00:00
2025-04-17 15:00:40,359 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
2025-04-17 15:00:40,360 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:00:40,360 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
2025-04-17 15:00:40,363 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:00:40,364 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 15:00:40,364 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:00:40,371 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:00:42,662 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1)
2025-04-17 15:00:42,667 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:00:39.847503+00:00, map_index=-1, run_start_date=2025-04-17 12:00:41.860784+00:00, run_end_date=2025-04-17 12:00:42.113955+00:00, run_duration=0.253171, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=213, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:00:40.362118+00:00, queued_by_job_id=201, pid=358277
2025-04-17 15:00:43,995 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
2025-04-17 15:00:43,995 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:00:43,996 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
2025-04-17 15:00:43,997 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:00:43,997 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
2025-04-17 15:00:43,997 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:00:44,002 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:00:45,922 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1)
2025-04-17 15:00:45,925 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T12:00:39.847503+00:00, map_index=-1, run_start_date=2025-04-17 12:00:45.356683+00:00, run_end_date=2025-04-17 12:00:45.469830+00:00, run_duration=0.113147, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=215, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 12:00:43.996468+00:00, queued_by_job_id=201, pid=358342
2025-04-17 15:00:56,907 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:01:32,676 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
2025-04-17 15:01:32,677 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:01:32,677 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
2025-04-17 15:01:32,680 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:01:32,681 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 15:01:32,681 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:01:32,688 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:01:34,528 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1)
2025-04-17 15:01:34,531 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:01:31.376092+00:00, map_index=-1, run_start_date=2025-04-17 12:01:33.902984+00:00, run_end_date=2025-04-17 12:01:34.058657+00:00, run_duration=0.155673, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=216, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:01:32.678439+00:00, queued_by_job_id=201, pid=358790
2025-04-17 15:01:34,577 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
2025-04-17 15:01:34,578 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:01:34,578 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
2025-04-17 15:01:34,579 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:01:34,579 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
2025-04-17 15:01:34,579 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:01:34,584 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:01:36,668 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1)
2025-04-17 15:01:36,673 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:01:31.376092+00:00, map_index=-1, run_start_date=2025-04-17 12:01:35.952753+00:00, run_end_date=2025-04-17 12:01:36.121403+00:00, run_duration=0.16865, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=217, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:01:34.578761+00:00, queued_by_job_id=201, pid=358819
2025-04-17 15:03:05,079 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
2025-04-17 15:03:05,079 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:03:05,079 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
2025-04-17 15:03:05,080 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:03:05,081 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 15:03:05,081 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:03:05,089 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:03:07,238 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1)
2025-04-17 15:03:07,241 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:03:04.921270+00:00, map_index=-1, run_start_date=2025-04-17 12:03:06.548579+00:00, run_end_date=2025-04-17 12:03:06.725075+00:00, run_duration=0.176496, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=219, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:03:05.080094+00:00, queued_by_job_id=201, pid=359672
2025-04-17 15:03:07,299 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
2025-04-17 15:03:07,299 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:03:07,299 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
2025-04-17 15:03:07,301 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:03:07,301 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
2025-04-17 15:03:07,301 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:03:07,307 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:03:09,537 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1)
2025-04-17 15:03:09,541 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:03:04.921270+00:00, map_index=-1, run_start_date=2025-04-17 12:03:08.833767+00:00, run_end_date=2025-04-17 12:03:08.992417+00:00, run_duration=0.15865, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=220, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:03:07.300464+00:00, queued_by_job_id=201, pid=359703
2025-04-17 15:03:13,473 INFO - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 12:03:04.921270+00:00: manual__2025-04-17T12:03:04.921270+00:00, state:running, queued_at: 2025-04-17 12:03:04.932886+00:00. externally triggered: True> successful
2025-04-17 15:03:13,474 INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 12:03:04.921270+00:00, run_id=manual__2025-04-17T12:03:04.921270+00:00, run_start_date=2025-04-17 12:03:05.051115+00:00, run_end_date=2025-04-17 12:03:13.474300+00:00, run_duration=8.423185, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 12:03:04.921270+00:00, data_interval_end=2025-04-17 12:03:04.921270+00:00, dag_hash=c0384d8f9877a9006c2905e35a0d6467
2025-04-17 15:04:28,094 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
2025-04-17 15:04:28,094 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:04:28,095 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
2025-04-17 15:04:28,096 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:04:28,097 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 15:04:28,097 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:04:28,106 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:04:30,400 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1)
2025-04-17 15:04:30,403 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:04:27.930098+00:00, map_index=-1, run_start_date=2025-04-17 12:04:29.677116+00:00, run_end_date=2025-04-17 12:04:29.866126+00:00, run_duration=0.18901, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=223, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:04:28.095513+00:00, queued_by_job_id=201, pid=360488
2025-04-17 15:04:30,574 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
2025-04-17 15:04:30,575 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:04:30,575 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
2025-04-17 15:04:30,576 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:04:30,577 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
2025-04-17 15:04:30,577 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:04:30,584 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:04:32,574 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1)
2025-04-17 15:04:32,577 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:04:27.930098+00:00, map_index=-1, run_start_date=2025-04-17 12:04:31.980684+00:00, run_end_date=2025-04-17 12:04:32.142964+00:00, run_duration=0.16228, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=224, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:04:30.576035+00:00, queued_by_job_id=201, pid=360520
2025-04-17 15:04:32,615 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
2025-04-17 15:04:32,616 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:04:32,616 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
2025-04-17 15:04:32,618 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:04:32,618 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
2025-04-17 15:04:32,618 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:04:32,622 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:04:34,580 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1)
2025-04-17 15:04:34,585 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T12:04:27.930098+00:00, map_index=-1, run_start_date=2025-04-17 12:04:33.894541+00:00, run_end_date=2025-04-17 12:04:34.069597+00:00, run_duration=0.175056, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=225, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 12:04:32.617180+00:00, queued_by_job_id=201, pid=360549
2025-04-17 15:05:34,563 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
2025-04-17 15:05:34,567 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:05:34,569 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
2025-04-17 15:05:34,575 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:05:34,577 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
2025-04-17 15:05:34,578 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:05:34,591 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:05:36,633 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1)
2025-04-17 15:05:36,636 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:05:31.793139+00:00, map_index=-1, run_start_date=2025-04-17 12:05:35.980766+00:00, run_end_date=2025-04-17 12:05:36.167110+00:00, run_duration=0.186344, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=228, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:05:34.570589+00:00, queued_by_job_id=201, pid=361145
2025-04-17 15:05:36,697 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
2025-04-17 15:05:36,698 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:05:36,698 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
2025-04-17 15:05:36,699 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:05:36,700 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
2025-04-17 15:05:36,700 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:05:36,705 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:05:38,577 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1)
2025-04-17 15:05:38,581 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T12:05:31.793139+00:00, map_index=-1, run_start_date=2025-04-17 12:05:38.009512+00:00, run_end_date=2025-04-17 12:05:38.159959+00:00, run_duration=0.150447, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=229, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 12:05:36.699189+00:00, queued_by_job_id=201, pid=361173
2025-04-17 15:05:38,625 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
2025-04-17 15:05:38,625 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 15:05:38,626 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
2025-04-17 15:05:38,627 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 15:05:38,628 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='group_by_smoker_region_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-04-17 15:05:38,628 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'group_by_smoker_region_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:05:38,632 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'group_by_smoker_region_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 15:05:40,588 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='group_by_smoker_region_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1)
2025-04-17 15:05:40,591 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=group_by_smoker_region_task, run_id=manual__2025-04-17T12:05:31.793139+00:00, map_index=-1, run_start_date=2025-04-17 12:05:39.998062+00:00, run_end_date=2025-04-17 12:05:40.164886+00:00, run_duration=0.166824, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=230, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-17 12:05:38.626590+00:00, queued_by_job_id=201, pid=361202
2025-04-17 15:05:57,075 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:10:57,129 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:15:57,174 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:20:57,220 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:25:57,291 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:30:57,341 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:35:57,523 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:40:57,581 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:45:57,644 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:50:57,714 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 15:55:57,768 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 16:00:57,879 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-17 16:04:13,711 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [scheduled]>
2025-04-17 16:04:13,712 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 16:04:13,713 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [scheduled]>
2025-04-17 16:04:13,714 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 16:04:13,721 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:04:12.630646+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 16:04:13,722 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:04:12.630646+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:04:13,727 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:04:12.630646+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:04:15,985 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:04:12.630646+00:00', try_number=1, map_index=-1)
2025-04-17 16:04:15,989 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_and_processing.read_csv_task, run_id=manual__2025-04-17T13:04:12.630646+00:00, map_index=-1, run_start_date=2025-04-17 13:04:15.282588+00:00, run_end_date=2025-04-17 13:04:15.457593+00:00, run_duration=0.175005, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=232, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 13:04:13.713853+00:00, queued_by_job_id=201, pid=391965
2025-04-17 16:04:18,157 ERROR - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 13:04:12.630646+00:00: manual__2025-04-17T13:04:12.630646+00:00, state:running, queued_at: 2025-04-17 13:04:12.644543+00:00. externally triggered: True> failed
2025-04-17 16:04:18,158 INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 13:04:12.630646+00:00, run_id=manual__2025-04-17T13:04:12.630646+00:00, run_start_date=2025-04-17 13:04:13.683328+00:00, run_end_date=2025-04-17 13:04:18.157985+00:00, run_duration=4.474657, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 13:04:12.630646+00:00, data_interval_end=2025-04-17 13:04:12.630646+00:00, dag_hash=dcb6c3164a16673204773a4cd3802965
2025-04-17 16:05:43,105 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
2025-04-17 16:05:43,105 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 16:05:43,105 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
2025-04-17 16:05:43,107 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 16:05:43,108 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
2025-04-17 16:05:43,108 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:05:43,120 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:05:45,087 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1)
2025-04-17 16:05:45,090 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_and_processing.read_csv_task, run_id=manual__2025-04-17T13:05:42.423164+00:00, map_index=-1, run_start_date=2025-04-17 13:05:44.458029+00:00, run_end_date=2025-04-17 13:05:44.616152+00:00, run_duration=0.158123, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=234, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 13:05:43.106328+00:00, queued_by_job_id=201, pid=392783
2025-04-17 16:05:45,135 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
2025-04-17 16:05:45,136 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 16:05:45,136 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
2025-04-17 16:05:45,137 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 16:05:45,138 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.remove_null_values_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
2025-04-17 16:05:45,138 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.remove_null_values_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:05:45,145 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.remove_null_values_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:05:47,329 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.remove_null_values_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1)
2025-04-17 16:05:47,333 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_and_processing.remove_null_values_task, run_id=manual__2025-04-17T13:05:42.423164+00:00, map_index=-1, run_start_date=2025-04-17 13:05:46.694837+00:00, run_end_date=2025-04-17 13:05:46.876529+00:00, run_duration=0.181692, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=235, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 13:05:45.137157+00:00, queued_by_job_id=201, pid=392816
2025-04-17 16:05:47,382 INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
2025-04-17 16:05:47,383 INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
2025-04-17 16:05:47,383 INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
2025-04-17 16:05:47,384 INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-04-17 16:05:47,385 INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
2025-04-17 16:05:47,385 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:05:47,390 INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
2025-04-17 16:05:49,480 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1)
2025-04-17 16:05:49,486 INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T13:05:42.423164+00:00, map_index=-1, run_start_date=2025-04-17 13:05:48.820309+00:00, run_end_date=2025-04-17 13:05:48.988494+00:00, run_duration=0.168185, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=236, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 13:05:47.383874+00:00, queued_by_job_id=201, pid=392857
2025-04-17 16:05:51,202 INFO - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 13:05:42.423164+00:00: manual__2025-04-17T13:05:42.423164+00:00, state:running, queued_at: 2025-04-17 13:05:42.446980+00:00. externally triggered: True> successful
2025-04-17 16:05:51,202 INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 13:05:42.423164+00:00, run_id=manual__2025-04-17T13:05:42.423164+00:00, run_start_date=2025-04-17 13:05:43.059211+00:00, run_end_date=2025-04-17 13:05:51.202556+00:00, run_duration=8.143345, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 13:05:42.423164+00:00, data_interval_end=2025-04-17 13:05:42.423164+00:00, dag_hash=dcb6c3164a16673204773a4cd3802965
2025-04-17 16:05:57,921 INFO - Adopting or resetting orphaned tasks for active dag runs
