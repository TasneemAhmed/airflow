[2025-04-17T14:55:56.793+0300] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2025-04-17T14:55:56.829+0300] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-04-17T14:55:56.830+0300] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-04-17T14:55:56.835+0300] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 355665
[2025-04-17T14:55:56.836+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T14:55:56.839+0300] {settings.py:63} INFO - Configured default timezone UTC
[2025-04-17T14:55:56.859+0300] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-04-17T14:59:27.107+0300] {dag.py:4180} INFO - Setting next_dagrun for executing_Python_pipeline_using_variable to None, run_after=None
[2025-04-17T14:59:27.158+0300] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
[2025-04-17T14:59:27.159+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T14:59:27.159+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 1/16 running and queued tasks
[2025-04-17T14:59:27.159+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
[2025-04-17T14:59:27.161+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [scheduled]>, <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T14:59:27.161+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='scheduled__2025-04-16T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T14:59:27.162+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'scheduled__2025-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T14:59:27.162+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T14:59:27.162+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T14:59:27.168+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'scheduled__2025-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T14:59:28.158+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T14:59:28.545+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task scheduled__2025-04-16T00:00:00+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T14:59:29.266+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T14:59:30.276+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T14:59:31.032+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T11:59:26.254367+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T14:59:33.177+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='scheduled__2025-04-16T00:00:00+00:00', try_number=1, map_index=-1)
[2025-04-17T14:59:33.178+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1)
[2025-04-17T14:59:33.186+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T11:59:26.254367+00:00, map_index=-1, run_start_date=2025-04-17 11:59:31.070713+00:00, run_end_date=2025-04-17 11:59:32.506624+00:00, run_duration=1.435911, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 11:59:27.160507+00:00, queued_by_job_id=201, pid=357553
[2025-04-17T14:59:33.190+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=scheduled__2025-04-16T00:00:00+00:00, map_index=-1, run_start_date=2025-04-17 11:59:28.581505+00:00, run_end_date=2025-04-17 11:59:28.798087+00:00, run_duration=0.216582, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 11:59:27.160507+00:00, queued_by_job_id=201, pid=357510
[2025-04-17T14:59:36.289+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
[2025-04-17T14:59:36.290+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 1/16 running and queued tasks
[2025-04-17T14:59:36.290+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>
[2025-04-17T14:59:36.292+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T14:59:36.292+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2025-04-17T14:59:36.293+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T14:59:36.301+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T11:59:26.254367+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T14:59:37.361+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T14:59:37.761+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T11:59:26.254367+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T14:59:38.576+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T11:59:26.254367+00:00', try_number=1, map_index=-1)
[2025-04-17T14:59:38.582+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T11:59:26.254367+00:00, map_index=-1, run_start_date=2025-04-17 11:59:37.809754+00:00, run_end_date=2025-04-17 11:59:38.027860+00:00, run_duration=0.218106, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 11:59:36.291316+00:00, queued_by_job_id=201, pid=357655
[2025-04-17T14:59:38.734+0300] {dagrun.py:823} ERROR - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-16 00:00:00+00:00: scheduled__2025-04-16T00:00:00+00:00, state:running, queued_at: 2025-04-17 11:59:27.096723+00:00. externally triggered: False> failed
[2025-04-17T14:59:38.735+0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-16 00:00:00+00:00, run_id=scheduled__2025-04-16T00:00:00+00:00, run_start_date=2025-04-17 11:59:27.119008+00:00, run_end_date=2025-04-17 11:59:38.735025+00:00, run_duration=11.616017, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-04-16 00:00:00+00:00, data_interval_end=2025-04-16 00:00:00+00:00, dag_hash=532dc0ac6ff5d311e9df40bcbefaa877
[2025-04-17T14:59:38.737+0300] {dag.py:4180} INFO - Setting next_dagrun for executing_Python_pipeline_using_variable to None, run_after=None
[2025-04-17T14:59:38.740+0300] {dagrun.py:823} ERROR - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 11:59:26.254367+00:00: manual__2025-04-17T11:59:26.254367+00:00, state:running, queued_at: 2025-04-17 11:59:26.289429+00:00. externally triggered: True> failed
[2025-04-17T14:59:38.740+0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 11:59:26.254367+00:00, run_id=manual__2025-04-17T11:59:26.254367+00:00, run_start_date=2025-04-17 11:59:27.119085+00:00, run_end_date=2025-04-17 11:59:38.740717+00:00, run_duration=11.621632, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 11:59:26.254367+00:00, data_interval_end=2025-04-17 11:59:26.254367+00:00, dag_hash=532dc0ac6ff5d311e9df40bcbefaa877
[2025-04-17T15:00:03.100+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [scheduled]>
[2025-04-17T15:00:03.100+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_Branching_tasks_with_Backfill has 0/16 running and queued tasks
[2025-04-17T15:00:03.100+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [scheduled]>
[2025-04-17T15:00:03.102+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:00:03.102+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_Branching_tasks_with_Backfill', task_id='task_choice', run_id='scheduled__2025-04-17T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 4 and queue default
[2025-04-17T15:00:03.102+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_Branching_tasks_with_Backfill', 'task_choice', 'scheduled__2025-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_cron_catchup_backfill.py']
[2025-04-17T15:00:03.108+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_Branching_tasks_with_Backfill', 'task_choice', 'scheduled__2025-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_cron_catchup_backfill.py']
[2025-04-17T15:00:03.961+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_cron_catchup_backfill.py
[2025-04-17T15:00:04.061+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_Branching_tasks_with_Backfill.task_choice scheduled__2025-04-17T00:00:00+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:00:04.800+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_Branching_tasks_with_Backfill', task_id='task_choice', run_id='scheduled__2025-04-17T00:00:00+00:00', try_number=1, map_index=-1)
[2025-04-17T15:00:04.804+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_Branching_tasks_with_Backfill, task_id=task_choice, run_id=scheduled__2025-04-17T00:00:00+00:00, map_index=-1, run_start_date=2025-04-17 12:00:04.101440+00:00, run_end_date=2025-04-17 12:00:04.245905+00:00, run_duration=0.144465, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=210, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-04-17 12:00:03.101358+00:00, queued_by_job_id=201, pid=357898
[2025-04-17T15:00:07.999+0300] {dagrun.py:854} INFO - Marking run <DagRun executing_Python_Branching_tasks_with_Backfill @ 2025-04-17 00:00:00+00:00: scheduled__2025-04-17T00:00:00+00:00, state:running, queued_at: 2025-04-17 12:00:01.390467+00:00. externally triggered: False> successful
[2025-04-17T15:00:08.000+0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_Python_Branching_tasks_with_Backfill, execution_date=2025-04-17 00:00:00+00:00, run_id=scheduled__2025-04-17T00:00:00+00:00, run_start_date=2025-04-17 12:00:01.418888+00:00, run_end_date=2025-04-17 12:00:08.000439+00:00, run_duration=6.581551, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-04-17 00:00:00+00:00, data_interval_end=2025-04-17 12:00:00+00:00, dag_hash=a7b8b386f36c9437c41d66ad90406d9d
[2025-04-17T15:00:08.007+0300] {dag.py:4180} INFO - Setting next_dagrun for executing_Python_Branching_tasks_with_Backfill to 2025-04-17 12:00:00+00:00, run_after=2025-04-18 00:00:00+00:00
[2025-04-17T15:00:40.359+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
[2025-04-17T15:00:40.360+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:00:40.360+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
[2025-04-17T15:00:40.363+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:00:40.364+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T15:00:40.364+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:00:40.371+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:00:41.434+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:00:41.811+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:00:39.847503+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:00:42.662+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1)
[2025-04-17T15:00:42.667+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:00:39.847503+00:00, map_index=-1, run_start_date=2025-04-17 12:00:41.860784+00:00, run_end_date=2025-04-17 12:00:42.113955+00:00, run_duration=0.253171, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=213, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:00:40.362118+00:00, queued_by_job_id=201, pid=358277
[2025-04-17T15:00:43.995+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
[2025-04-17T15:00:43.995+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:00:43.996+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>
[2025-04-17T15:00:43.997+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:00:43.997+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2025-04-17T15:00:43.997+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:00:44.002+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:00:39.847503+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:00:44.956+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:00:45.322+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:00:39.847503+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:00:45.922+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:00:39.847503+00:00', try_number=1, map_index=-1)
[2025-04-17T15:00:45.925+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T12:00:39.847503+00:00, map_index=-1, run_start_date=2025-04-17 12:00:45.356683+00:00, run_end_date=2025-04-17 12:00:45.469830+00:00, run_duration=0.113147, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=215, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 12:00:43.996468+00:00, queued_by_job_id=201, pid=358342
[2025-04-17T15:00:56.907+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:01:32.676+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
[2025-04-17T15:01:32.677+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:01:32.677+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
[2025-04-17T15:01:32.680+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:01:32.681+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T15:01:32.681+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:01:32.688+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:01:33.477+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:01:33.861+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:01:31.376092+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:01:34.528+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1)
[2025-04-17T15:01:34.531+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:01:31.376092+00:00, map_index=-1, run_start_date=2025-04-17 12:01:33.902984+00:00, run_end_date=2025-04-17 12:01:34.058657+00:00, run_duration=0.155673, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=216, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:01:32.678439+00:00, queued_by_job_id=201, pid=358790
[2025-04-17T15:01:34.577+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
[2025-04-17T15:01:34.578+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:01:34.578+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>
[2025-04-17T15:01:34.579+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:01:34.579+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-04-17T15:01:34.579+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:01:34.584+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:01:31.376092+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:01:35.488+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:01:35.909+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:01:31.376092+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:01:36.668+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:01:31.376092+00:00', try_number=1, map_index=-1)
[2025-04-17T15:01:36.673+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:01:31.376092+00:00, map_index=-1, run_start_date=2025-04-17 12:01:35.952753+00:00, run_end_date=2025-04-17 12:01:36.121403+00:00, run_duration=0.16865, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=217, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:01:34.578761+00:00, queued_by_job_id=201, pid=358819
[2025-04-17T15:03:05.079+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
[2025-04-17T15:03:05.079+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:03:05.079+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
[2025-04-17T15:03:05.080+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:03:05.081+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T15:03:05.081+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:03:05.089+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:03:06.124+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:03:06.508+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:03:04.921270+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:03:07.238+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1)
[2025-04-17T15:03:07.241+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:03:04.921270+00:00, map_index=-1, run_start_date=2025-04-17 12:03:06.548579+00:00, run_end_date=2025-04-17 12:03:06.725075+00:00, run_duration=0.176496, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=219, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:03:05.080094+00:00, queued_by_job_id=201, pid=359672
[2025-04-17T15:03:07.299+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
[2025-04-17T15:03:07.299+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:03:07.299+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>
[2025-04-17T15:03:07.301+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:03:07.301+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-04-17T15:03:07.301+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:03:07.307+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:03:04.921270+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:03:08.364+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:03:08.797+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:03:04.921270+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:03:09.537+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:03:04.921270+00:00', try_number=1, map_index=-1)
[2025-04-17T15:03:09.541+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:03:04.921270+00:00, map_index=-1, run_start_date=2025-04-17 12:03:08.833767+00:00, run_end_date=2025-04-17 12:03:08.992417+00:00, run_duration=0.15865, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=220, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:03:07.300464+00:00, queued_by_job_id=201, pid=359703
[2025-04-17T15:03:13.473+0300] {dagrun.py:854} INFO - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 12:03:04.921270+00:00: manual__2025-04-17T12:03:04.921270+00:00, state:running, queued_at: 2025-04-17 12:03:04.932886+00:00. externally triggered: True> successful
[2025-04-17T15:03:13.474+0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 12:03:04.921270+00:00, run_id=manual__2025-04-17T12:03:04.921270+00:00, run_start_date=2025-04-17 12:03:05.051115+00:00, run_end_date=2025-04-17 12:03:13.474300+00:00, run_duration=8.423185, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 12:03:04.921270+00:00, data_interval_end=2025-04-17 12:03:04.921270+00:00, dag_hash=c0384d8f9877a9006c2905e35a0d6467
[2025-04-17T15:04:28.094+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
[2025-04-17T15:04:28.094+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:04:28.095+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
[2025-04-17T15:04:28.096+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:04:28.097+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T15:04:28.097+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:04:28.106+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_csv_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:04:29.233+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:04:29.640+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_csv_task manual__2025-04-17T12:04:27.930098+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:04:30.400+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_csv_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1)
[2025-04-17T15:04:30.403+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_csv_task, run_id=manual__2025-04-17T12:04:27.930098+00:00, map_index=-1, run_start_date=2025-04-17 12:04:29.677116+00:00, run_end_date=2025-04-17 12:04:29.866126+00:00, run_duration=0.18901, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=223, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 12:04:28.095513+00:00, queued_by_job_id=201, pid=360488
[2025-04-17T15:04:30.574+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
[2025-04-17T15:04:30.575+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:04:30.575+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
[2025-04-17T15:04:30.576+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:04:30.577+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-04-17T15:04:30.577+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:04:30.584+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:04:31.547+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:04:31.945+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:04:27.930098+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:04:32.574+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1)
[2025-04-17T15:04:32.577+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:04:27.930098+00:00, map_index=-1, run_start_date=2025-04-17 12:04:31.980684+00:00, run_end_date=2025-04-17 12:04:32.142964+00:00, run_duration=0.16228, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=224, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:04:30.576035+00:00, queued_by_job_id=201, pid=360520
[2025-04-17T15:04:32.615+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
[2025-04-17T15:04:32.616+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:04:32.616+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>
[2025-04-17T15:04:32.618+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:04:32.618+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2025-04-17T15:04:32.618+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:04:32.622+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:04:27.930098+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:04:33.432+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:04:33.854+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:04:27.930098+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:04:34.580+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:04:27.930098+00:00', try_number=1, map_index=-1)
[2025-04-17T15:04:34.585+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T12:04:27.930098+00:00, map_index=-1, run_start_date=2025-04-17 12:04:33.894541+00:00, run_end_date=2025-04-17 12:04:34.069597+00:00, run_duration=0.175056, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=225, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 12:04:32.617180+00:00, queued_by_job_id=201, pid=360549
[2025-04-17T15:05:34.563+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
[2025-04-17T15:05:34.567+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:05:34.569+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
[2025-04-17T15:05:34.575+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:05:34.577+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-04-17T15:05:34.578+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:05:34.591+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'remove_null_values_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:05:35.544+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:05:35.928+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.remove_null_values_task manual__2025-04-17T12:05:31.793139+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:05:36.633+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='remove_null_values_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1)
[2025-04-17T15:05:36.636+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=remove_null_values_task, run_id=manual__2025-04-17T12:05:31.793139+00:00, map_index=-1, run_start_date=2025-04-17 12:05:35.980766+00:00, run_end_date=2025-04-17 12:05:36.167110+00:00, run_duration=0.186344, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=228, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 12:05:34.570589+00:00, queued_by_job_id=201, pid=361145
[2025-04-17T15:05:36.697+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
[2025-04-17T15:05:36.698+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:05:36.698+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
[2025-04-17T15:05:36.699+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:05:36.700+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2025-04-17T15:05:36.700+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:05:36.705+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:05:37.609+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:05:37.968+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T12:05:31.793139+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:05:38.577+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1)
[2025-04-17T15:05:38.581+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T12:05:31.793139+00:00, map_index=-1, run_start_date=2025-04-17 12:05:38.009512+00:00, run_end_date=2025-04-17 12:05:38.159959+00:00, run_duration=0.150447, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=229, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 12:05:36.699189+00:00, queued_by_job_id=201, pid=361173
[2025-04-17T15:05:38.625+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
[2025-04-17T15:05:38.625+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T15:05:38.626+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>
[2025-04-17T15:05:38.627+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T15:05:38.628+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='group_by_smoker_region_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-04-17T15:05:38.628+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'group_by_smoker_region_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:05:38.632+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'group_by_smoker_region_task', 'manual__2025-04-17T12:05:31.793139+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T15:05:39.551+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T15:05:39.960+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.group_by_smoker_region_task manual__2025-04-17T12:05:31.793139+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T15:05:40.588+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='group_by_smoker_region_task', run_id='manual__2025-04-17T12:05:31.793139+00:00', try_number=1, map_index=-1)
[2025-04-17T15:05:40.591+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=group_by_smoker_region_task, run_id=manual__2025-04-17T12:05:31.793139+00:00, map_index=-1, run_start_date=2025-04-17 12:05:39.998062+00:00, run_end_date=2025-04-17 12:05:40.164886+00:00, run_duration=0.166824, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=230, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-17 12:05:38.626590+00:00, queued_by_job_id=201, pid=361202
[2025-04-17T15:05:57.075+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:10:57.129+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:15:57.174+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:20:57.220+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:25:57.291+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:30:57.341+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:35:57.523+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:40:57.581+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:45:57.644+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:50:57.714+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T15:55:57.768+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T16:00:31.039+0300] {manager.py:537} INFO - DAG executing_Python_pipeline_using_variable is missing and will be deactivated.
[2025-04-17T16:00:31.054+0300] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-04-17T16:00:31.071+0300] {manager.py:553} INFO - Deleted DAG executing_Python_pipeline_using_variable in serialized_dag table
[2025-04-17T16:00:57.879+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-17T16:04:13.711+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [scheduled]>
[2025-04-17T16:04:13.712+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T16:04:13.713+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [scheduled]>
[2025-04-17T16:04:13.714+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T16:04:13.721+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:04:12.630646+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T16:04:13.722+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:04:12.630646+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:04:13.727+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:04:12.630646+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:04:14.735+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T16:04:15.219+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:04:12.630646+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T16:04:15.985+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:04:12.630646+00:00', try_number=1, map_index=-1)
[2025-04-17T16:04:15.989+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_and_processing.read_csv_task, run_id=manual__2025-04-17T13:04:12.630646+00:00, map_index=-1, run_start_date=2025-04-17 13:04:15.282588+00:00, run_end_date=2025-04-17 13:04:15.457593+00:00, run_duration=0.175005, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=232, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 13:04:13.713853+00:00, queued_by_job_id=201, pid=391965
[2025-04-17T16:04:18.157+0300] {dagrun.py:823} ERROR - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 13:04:12.630646+00:00: manual__2025-04-17T13:04:12.630646+00:00, state:running, queued_at: 2025-04-17 13:04:12.644543+00:00. externally triggered: True> failed
[2025-04-17T16:04:18.158+0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 13:04:12.630646+00:00, run_id=manual__2025-04-17T13:04:12.630646+00:00, run_start_date=2025-04-17 13:04:13.683328+00:00, run_end_date=2025-04-17 13:04:18.157985+00:00, run_duration=4.474657, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 13:04:12.630646+00:00, data_interval_end=2025-04-17 13:04:12.630646+00:00, dag_hash=dcb6c3164a16673204773a4cd3802965
[2025-04-17T16:05:43.105+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
[2025-04-17T16:05:43.105+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T16:05:43.105+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
[2025-04-17T16:05:43.107+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T16:05:43.108+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 8 and queue default
[2025-04-17T16:05:43.108+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:05:43.120+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.read_csv_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:05:43.963+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T16:05:44.413+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.read_csv_task manual__2025-04-17T13:05:42.423164+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T16:05:45.087+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.read_csv_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1)
[2025-04-17T16:05:45.090+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_and_processing.read_csv_task, run_id=manual__2025-04-17T13:05:42.423164+00:00, map_index=-1, run_start_date=2025-04-17 13:05:44.458029+00:00, run_end_date=2025-04-17 13:05:44.616152+00:00, run_duration=0.158123, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=234, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2025-04-17 13:05:43.106328+00:00, queued_by_job_id=201, pid=392783
[2025-04-17T16:05:45.135+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
[2025-04-17T16:05:45.136+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T16:05:45.136+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
[2025-04-17T16:05:45.137+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T16:05:45.138+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.remove_null_values_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-04-17T16:05:45.138+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.remove_null_values_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:05:45.145+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'read_and_processing.remove_null_values_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:05:46.231+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T16:05:46.652+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.read_and_processing.remove_null_values_task manual__2025-04-17T13:05:42.423164+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T16:05:47.329+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='read_and_processing.remove_null_values_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1)
[2025-04-17T16:05:47.333+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=read_and_processing.remove_null_values_task, run_id=manual__2025-04-17T13:05:42.423164+00:00, map_index=-1, run_start_date=2025-04-17 13:05:46.694837+00:00, run_end_date=2025-04-17 13:05:46.876529+00:00, run_duration=0.181692, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=235, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2025-04-17 13:05:45.137157+00:00, queued_by_job_id=201, pid=392816
[2025-04-17T16:05:47.382+0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
[2025-04-17T16:05:47.383+0300] {scheduler_job_runner.py:507} INFO - DAG executing_Python_pipeline_using_variable has 0/16 running and queued tasks
[2025-04-17T16:05:47.383+0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>
[2025-04-17T16:05:47.384+0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-04-17T16:05:47.385+0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2025-04-17T16:05:47.385+0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:05:47.390+0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_Python_pipeline_using_variable', 'branching_task', 'manual__2025-04-17T13:05:42.423164+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_branching_using_variable.py']
[2025-04-17T16:05:48.402+0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/tasneemahmed24/airflow/dags/execute_branching_using_variable.py
[2025-04-17T16:05:48.771+0300] {task_command.py:467} INFO - Running <TaskInstance: executing_Python_pipeline_using_variable.branching_task manual__2025-04-17T13:05:42.423164+00:00 [queued]> on host DESKTOP-1SBO1JU.
[2025-04-17T16:05:49.480+0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_Python_pipeline_using_variable', task_id='branching_task', run_id='manual__2025-04-17T13:05:42.423164+00:00', try_number=1, map_index=-1)
[2025-04-17T16:05:49.486+0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=executing_Python_pipeline_using_variable, task_id=branching_task, run_id=manual__2025-04-17T13:05:42.423164+00:00, map_index=-1, run_start_date=2025-04-17 13:05:48.820309+00:00, run_end_date=2025-04-17 13:05:48.988494+00:00, run_duration=0.168185, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=236, pool=default_pool, queue=default, priority_weight=6, operator=BranchPythonOperator, queued_dttm=2025-04-17 13:05:47.383874+00:00, queued_by_job_id=201, pid=392857
[2025-04-17T16:05:51.202+0300] {dagrun.py:854} INFO - Marking run <DagRun executing_Python_pipeline_using_variable @ 2025-04-17 13:05:42.423164+00:00: manual__2025-04-17T13:05:42.423164+00:00, state:running, queued_at: 2025-04-17 13:05:42.446980+00:00. externally triggered: True> successful
[2025-04-17T16:05:51.202+0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_Python_pipeline_using_variable, execution_date=2025-04-17 13:05:42.423164+00:00, run_id=manual__2025-04-17T13:05:42.423164+00:00, run_start_date=2025-04-17 13:05:43.059211+00:00, run_end_date=2025-04-17 13:05:51.202556+00:00, run_duration=8.143345, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-17 13:05:42.423164+00:00, data_interval_end=2025-04-17 13:05:42.423164+00:00, dag_hash=dcb6c3164a16673204773a4cd3802965
[2025-04-17T16:05:57.921+0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
