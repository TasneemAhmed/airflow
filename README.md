Certainly! Here's a comprehensive `README.md` for your [Airflow GitHub repository](https://github.com/TasneemAhmed/airflow), aligning with the concepts covered in the [LinkedIn Learning course on Apache Airflow](https://www.linkedin.com/learning/learning-apache-airflow).

Inside CourseMaterials/ you will find the exercise files attached to the LinkedIn course.
## 🏅 Certification

You can view my certificate of completion for the course [Learning Apache Airflow](https://www.linkedin.com/learning/learning-apache-airflow) here:

📄 [View Certificate](CourseMaterials/my_airflow_certification.pdf)

---

# Apache Airflow Learning Repository

This repository contains hands-on examples and DAGs (Directed Acyclic Graphs) developed while following the [LinkedIn Learning course: Learning Apache Airflow](https://www.linkedin.com/learning/learning-apache-airflow). It covers fundamental concepts of Airflow, including architecture, task creation, dependencies, XComs, TaskGroups, scheduling, catchup, and backfill.

## 📁 Repository Structure

```
airflow/
├── CourseMaterials/
│   ├── Ex_Files_Learning_Apache_Airflow.zip
├── dags/
│   ├── sql_operators.py
│   ├── sample_hello_world.py
│   ├── execute_python_pipeline.py
│   ├── execute_python_operator.py
│   ├── execute_multiple_tasks.py
│   └── execute_cron_catchup_backfill.py
|   └── execute_branching.py
|   └── execute_branching_using_variable.py
|   └── cross_task_communication.py
├── logs/
├── airflow.cfg
└── README.md
```

### `dags/`
Contains all the DAGs created during the course

- **`execute_python_operator.py`**:Demonstrates a simple DAG using the PythonOperator
- **`sql_operators.py`**:Illustrates task creation using the SQLiteOperator
- **`execute_multiple_tasks.py`**:Shows how to use the BashOperator to execute shell commands with multiple tasks and set dependencies between them.
- **`cross_task_communication.py`**:Explains how to pass data between tasks using XComs
- **`execute_branching_using_variable.py`**:Demonstrates organizing tasks into TaskGroups for better visualization, branching between tasks, running specific task based on the value of the variable
   and set labels on edges between nodes/tasks.
- **`execute_cron_catchup_backfill.py`**:Covers scheduling concepts including cron expressions, catchup, and backfill

### `logs/`
Stores logs generated by Airflow for monitoring and debugging purposes

### `airflow.cfg`
The Airflow configuration file contains settings for the environment

## 🧠 Concepts Covered
The following Airflow concepts are explored through a practical example:

- **Airflow Architecture** Understanding components like the scheduler, web server, and metadata database.
  ![image](https://github.com/user-attachments/assets/2adc4698-4eb8-4577-8fe3-9b17b768b0f2)

- **Task Creation** Using operators such as PythonOperator, BashOperator, and SQLiteOperato.
- **Dependencies** Modeling workflows by setting task dependencies.
- **XComs** Passing data between tasks using Airflow's cross-communication mechanism.
- **TaskGroups** Organizing tasks into logical groups for better DAG visualization.
  ![image](https://github.com/user-attachments/assets/b6b3693d-179c-4499-a92d-6357cb8e1f0e)

- **Scheduling** Implementing cron expressions, understanding catchup and backfill feature.

## 🖥️ Setting Up Apache Airflow on Windows

Apache Airflow is designed to run on Unix-based systems. To set it up on Windows, you can use the Windows Subsystem for Linux (WS).

### Prerequisites

- **Enable WSL**:
   - Go to "Turn Windows features on or off" and enable "Windows Subsystem for Linu".
- **Install a Linux Distribution**:
   - From the Microsoft Store, install a distribution like Ubunu.
- **Install Python and Pip**:
   - Within the WSL terminl:
      ```bash
      sudo apt update
      sudo apt install python3 python3-pip
      ```

### Installing Airflow

1. **Set Up a Virtual Environment**:
   ```bash
   sudo apt install python3-venv
   python3 -m venv airflow_env
   source airflow_env/bin/activate
   ```

2. **Set the AIRFLOW_HOME Environment Variable**:
   ```bash
   export AIRFLOW_HOME=~/airflow
   mkdir -p $AIRFLOW_HOME/dags
   ```

3. **Install Apache Airflow**:
   ```bash
   pip install apache-airflow
   ```

4. **Initialize the Database**:
   ```bash
   airflow db init
   ```

5. **Create an Admin User**:
   ```bash
   airflow users create \
     --username admin \
     --firstname Admin \
     --lastname User \
     --role Admin \
     --email admin@example.com
   ```

6. **Start the Scheduler**:
   ```bash
   airflow scheduler
   ```

7. **Start the Web Server**:
   ```bash
   airflow webserver --port 8080
   ```


Access the Airflow UI by navigating to `http://localhost:8080` in your web browsr.

For a detailed guide on installing Apache Airflow on Windows without Docker, refer to this [freeCodeCamp article](https://www.freecodecamp.org/news/install-apache-airflow-on-windows-without-docker).

---

Feel free to explore the DAGs and modify them to deepen your understanding of Apache Airflow. Happy learning! 
